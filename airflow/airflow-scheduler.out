[[34m2025-07-08T08:36:47.851+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2025-07-08T08:36:47.854+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2025-07-08T08:36:47.863+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 2655[0m
[[34m2025-07-08T08:36:47.866+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T08:36:47.873+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2025-07-08T08:36:47.913+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-07-08T08:36:54.620+0000[0m] {[34mserve_logs.py:[0m60} WARNING[0m - The Authorization header is missing: Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Host: localhost:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9
Cache-Control: max-age=0
Referer: https://urban-halibut-vwgxj676qjqh6w44.github.dev/
X-Request-Id: 3254249dd0205479b707d1a30a7df7ca
X-Real-Ip: 203.126.125.83
X-Forwarded-Port: 443
X-Forwarded-Scheme: https
X-Original-Uri: /
X-Scheme: https
Sec-Fetch-Site: same-site
Sec-Fetch-Mode: navigate
Sec-Fetch-Dest: document
Sec-Ch-Ua: "Not)A;Brand";v="8", "Chromium";v="138", "Microsoft Edge";v="138"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Platform: "Windows"
Priority: u=0, i
X-Original-Proto: https
X-Forwarded-Proto: https
X-Forwarded-Host: urban-halibut-vwgxj676qjqh6w44-8793.app.github.dev
X-Forwarded-For: 203.126.125.83
Proxy-Connection: Keep-Alive

.[0m
[[34m2025-07-08T08:36:54.621+0000[0m] {[34mserve_logs.py:[0m104} WARNING[0m - Unknown error[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/utils/serve_logs.py", line 61, in validate_pre_signed_url
    abort(403)
  File "/home/vscode/.local/lib/python3.10/site-packages/flask/helpers.py", line 310, in abort
    current_app.aborter(code, *args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/werkzeug/exceptions.py", line 864, in __call__
    raise self.mapping[code](*args, **kwargs)
werkzeug.exceptions.Forbidden: 403 Forbidden: You don't have the permission to access the requested resource. It is either read-protected or not readable by the server.[0m
[[34m2025-07-08T08:36:54.738+0000[0m] {[34mserve_logs.py:[0m60} WARNING[0m - The Authorization header is missing: Accept: image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8
Host: localhost:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36 Edg/138.0.0.0
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9
Referer: https://urban-halibut-vwgxj676qjqh6w44-8793.app.github.dev/
X-Request-Id: b6bf168971dbca2e0d3d3be9c1e12622
X-Real-Ip: 203.126.125.83
X-Forwarded-Port: 443
X-Forwarded-Scheme: https
X-Original-Uri: /favicon.ico
X-Scheme: https
Sec-Ch-Ua-Platform: "Windows"
Sec-Ch-Ua: "Not)A;Brand";v="8", "Chromium";v="138", "Microsoft Edge";v="138"
Sec-Ch-Ua-Mobile: ?0
Sec-Fetch-Site: same-origin
Sec-Fetch-Mode: no-cors
Sec-Fetch-Dest: image
Priority: u=1, i
X-Original-Proto: https
X-Forwarded-Proto: https
X-Forwarded-Host: urban-halibut-vwgxj676qjqh6w44-8793.app.github.dev
X-Forwarded-For: 203.126.125.83
Proxy-Connection: Keep-Alive

.[0m
[[34m2025-07-08T08:36:54.739+0000[0m] {[34mserve_logs.py:[0m104} WARNING[0m - Unknown error[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/utils/serve_logs.py", line 61, in validate_pre_signed_url
    abort(403)
  File "/home/vscode/.local/lib/python3.10/site-packages/flask/helpers.py", line 310, in abort
    current_app.aborter(code, *args, **kwargs)
  File "/home/vscode/.local/lib/python3.10/site-packages/werkzeug/exceptions.py", line 864, in __call__
    raise self.mapping[code](*args, **kwargs)
werkzeug.exceptions.Forbidden: 403 Forbidden: You don't have the permission to access the requested resource. It is either read-protected or not readable by the server.[0m
[[34m2025-07-08T08:41:48.093+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T08:46:48.229+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T08:51:48.387+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T08:56:48.545+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:01:48.687+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:06:48.826+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:09:59.145+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:09:58.618153+00:00 [scheduled]>[0m
[[34m2025-07-08T09:09:59.145+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:09:59.145+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:09:58.618153+00:00 [scheduled]>[0m
[[34m2025-07-08T09:09:59.147+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:09:58.618153+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:09:59.147+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:09:58.618153+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:09:59.151+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:09:58.618153+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:10:00.001+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:10:00.624+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:10:00.625+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:00.652+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:00.712+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:10:00.734+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:09:58.618153+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:10:02.414+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:09:58.618153+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:10:02.420+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:09:58.618153+00:00, map_index=-1, run_start_date=2025-07-08 09:10:00.781344+00:00, run_end_date=2025-07-08 09:10:02.052960+00:00, run_duration=1.271616, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:09:59.146357+00:00, queued_by_job_id=1, pid=17812[0m
[[34m2025-07-08T09:10:02.591+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:09:58.618153+00:00: manual__2025-07-08T09:09:58.618153+00:00, state:running, queued_at: 2025-07-08 09:09:58.641823+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:10:02.592+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:09:58.618153+00:00, run_id=manual__2025-07-08T09:09:58.618153+00:00, run_start_date=2025-07-08 09:09:59.121036+00:00, run_end_date=2025-07-08 09:10:02.592072+00:00, run_duration=3.471036, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:09:58.618153+00:00, data_interval_end=2025-07-08 09:09:58.618153+00:00, dag_hash=0c10bebcb890d51e1a762e7cd91cdca3[0m
[[34m2025-07-08T09:10:11.868+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:10.062731+00:00 [scheduled]>[0m
[[34m2025-07-08T09:10:11.868+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:10:11.868+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:10.062731+00:00 [scheduled]>[0m
[[34m2025-07-08T09:10:11.869+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:10:10.062731+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:10:11.870+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:10:10.062731+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:10:11.873+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:10:10.062731+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:10:12.592+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:10:13.117+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:10:13.117+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:13.140+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:13.178+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:10:13.199+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:10.062731+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:10:14.747+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:10:10.062731+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:10:14.750+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:10:10.062731+00:00, map_index=-1, run_start_date=2025-07-08 09:10:13.234594+00:00, run_end_date=2025-07-08 09:10:14.429165+00:00, run_duration=1.194571, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:10:11.868999+00:00, queued_by_job_id=1, pid=17898[0m
[[34m2025-07-08T09:10:14.917+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:10:10.062731+00:00: manual__2025-07-08T09:10:10.062731+00:00, state:running, queued_at: 2025-07-08 09:10:10.068964+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:10:14.918+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:10:10.062731+00:00, run_id=manual__2025-07-08T09:10:10.062731+00:00, run_start_date=2025-07-08 09:10:11.850545+00:00, run_end_date=2025-07-08 09:10:14.918241+00:00, run_duration=3.067696, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:10:10.062731+00:00, data_interval_end=2025-07-08 09:10:10.062731+00:00, dag_hash=0c10bebcb890d51e1a762e7cd91cdca3[0m
[[34m2025-07-08T09:10:21.043+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:20.745278+00:00 [scheduled]>[0m
[[34m2025-07-08T09:10:21.043+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:10:21.044+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:20.745278+00:00 [scheduled]>[0m
[[34m2025-07-08T09:10:21.045+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:10:20.745278+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:10:21.045+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:10:20.745278+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:10:21.049+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:10:20.745278+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:10:21.827+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:10:22.445+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:10:22.446+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:22.477+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:10:22.516+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:10:22.539+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:10:20.745278+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:10:24.207+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:10:20.745278+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:10:24.211+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:10:20.745278+00:00, map_index=-1, run_start_date=2025-07-08 09:10:22.575401+00:00, run_end_date=2025-07-08 09:10:23.875733+00:00, run_duration=1.300332, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:10:21.044489+00:00, queued_by_job_id=1, pid=17966[0m
[[34m2025-07-08T09:10:24.375+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:10:20.745278+00:00: manual__2025-07-08T09:10:20.745278+00:00, state:running, queued_at: 2025-07-08 09:10:20.751099+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:10:24.375+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:10:20.745278+00:00, run_id=manual__2025-07-08T09:10:20.745278+00:00, run_start_date=2025-07-08 09:10:21.025588+00:00, run_end_date=2025-07-08 09:10:24.375609+00:00, run_duration=3.350021, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:10:20.745278+00:00, data_interval_end=2025-07-08 09:10:20.745278+00:00, dag_hash=0c10bebcb890d51e1a762e7cd91cdca3[0m
[[34m2025-07-08T09:11:48.984+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:12:00.826+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:11:59.845766+00:00 [scheduled]>[0m
[[34m2025-07-08T09:12:00.826+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:12:00.826+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:11:59.845766+00:00 [scheduled]>[0m
[[34m2025-07-08T09:12:00.828+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:11:59.845766+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:12:00.828+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:11:59.845766+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:12:00.832+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:11:59.845766+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:12:01.578+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:12:02.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:12:02.184+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:12:02.207+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:12:02.248+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:12:02.272+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:11:59.845766+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:12:03.833+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:11:59.845766+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:12:03.836+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:11:59.845766+00:00, map_index=-1, run_start_date=2025-07-08 09:12:02.316113+00:00, run_end_date=2025-07-08 09:12:03.510119+00:00, run_duration=1.194006, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:12:00.827374+00:00, queued_by_job_id=1, pid=18676[0m
[[34m2025-07-08T09:12:04.106+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:11:59.845766+00:00: manual__2025-07-08T09:11:59.845766+00:00, state:running, queued_at: 2025-07-08 09:11:59.850110+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:12:04.107+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:11:59.845766+00:00, run_id=manual__2025-07-08T09:11:59.845766+00:00, run_start_date=2025-07-08 09:12:00.807689+00:00, run_end_date=2025-07-08 09:12:04.107079+00:00, run_duration=3.29939, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:11:59.845766+00:00, data_interval_end=2025-07-08 09:11:59.845766+00:00, dag_hash=ed9e1081cec60647beb3e959c1f953d8[0m
[[34m2025-07-08T09:13:30.182+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:13:29.629678+00:00 [scheduled]>[0m
[[34m2025-07-08T09:13:30.182+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:13:30.182+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:13:29.629678+00:00 [scheduled]>[0m
[[34m2025-07-08T09:13:30.184+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:13:29.629678+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:13:30.184+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:13:29.629678+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:13:30.188+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:13:29.629678+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:13:30.909+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:13:31.465+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:13:31.466+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:13:31.489+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:13:31.528+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:13:31.554+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:13:29.629678+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:13:33.148+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:13:29.629678+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:13:33.154+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:13:29.629678+00:00, map_index=-1, run_start_date=2025-07-08 09:13:31.609172+00:00, run_end_date=2025-07-08 09:13:32.801217+00:00, run_duration=1.192045, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:13:30.183286+00:00, queued_by_job_id=1, pid=19305[0m
[[34m2025-07-08T09:13:33.326+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:13:29.629678+00:00: manual__2025-07-08T09:13:29.629678+00:00, state:running, queued_at: 2025-07-08 09:13:29.635009+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:13:33.326+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:13:29.629678+00:00, run_id=manual__2025-07-08T09:13:29.629678+00:00, run_start_date=2025-07-08 09:13:30.163614+00:00, run_end_date=2025-07-08 09:13:33.326565+00:00, run_duration=3.162951, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:13:29.629678+00:00, data_interval_end=2025-07-08 09:13:29.629678+00:00, dag_hash=ed9e1081cec60647beb3e959c1f953d8[0m
[[34m2025-07-08T09:14:04.087+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:14:03.775829+00:00 [scheduled]>[0m
[[34m2025-07-08T09:14:04.087+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:14:04.087+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2025-07-08T09:14:03.775829+00:00 [scheduled]>[0m
[[34m2025-07-08T09:14:04.089+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:14:03.775829+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:14:04.089+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:14:03.775829+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:14:04.093+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2025-07-08T09:14:03.775829+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2025-07-08T09:14:04.862+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2025-07-08T09:14:05.457+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m39} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/vscode/.local/lib/python3.10/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 37, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-07-08T09:14:05.458+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:14:05.481+0000[0m] {[34mexample_kubernetes_executor.py:[0m41} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-07-08T09:14:05.522+0000[0m] {[34mworkday.py:[0m36} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-07-08T09:14:05.550+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2025-07-08T09:14:03.775829+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:14:07.184+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2025-07-08T09:14:03.775829+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:14:07.188+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2025-07-08T09:14:03.775829+00:00, map_index=-1, run_start_date=2025-07-08 09:14:05.609297+00:00, run_end_date=2025-07-08 09:14:06.808271+00:00, run_duration=1.198974, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:14:04.088159+00:00, queued_by_job_id=1, pid=19566[0m
[[34m2025-07-08T09:14:07.475+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2025-07-08 09:14:03.775829+00:00: manual__2025-07-08T09:14:03.775829+00:00, state:running, queued_at: 2025-07-08 09:14:03.784466+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:14:07.476+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2025-07-08 09:14:03.775829+00:00, run_id=manual__2025-07-08T09:14:03.775829+00:00, run_start_date=2025-07-08 09:14:04.067504+00:00, run_end_date=2025-07-08 09:14:07.476245+00:00, run_duration=3.408741, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:14:03.775829+00:00, data_interval_end=2025-07-08 09:14:03.775829+00:00, dag_hash=ed9e1081cec60647beb3e959c1f953d8[0m
[[34m2025-07-08T09:16:49.120+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:21:49.188+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:26:49.999+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:28:33.291+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: transform_dag.transform_task manual__2025-07-08T09:28:32.798411+00:00 [scheduled]>[0m
[[34m2025-07-08T09:28:33.291+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG transform_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:28:33.292+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: transform_dag.transform_task manual__2025-07-08T09:28:32.798411+00:00 [scheduled]>[0m
[[34m2025-07-08T09:28:33.294+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='transform_dag', task_id='transform_task', run_id='manual__2025-07-08T09:28:32.798411+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:28:33.294+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'transform_dag', 'transform_task', 'manual__2025-07-08T09:28:32.798411+00:00', '--local', '--subdir', 'DAGS_FOLDER/transform_dag.py'][0m
[[34m2025-07-08T09:28:33.297+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'transform_dag', 'transform_task', 'manual__2025-07-08T09:28:32.798411+00:00', '--local', '--subdir', 'DAGS_FOLDER/transform_dag.py'][0m
[[34m2025-07-08T09:28:34.075+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/transform_dag.py[0m
[[34m2025-07-08T09:28:34.885+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: transform_dag.transform_task manual__2025-07-08T09:28:32.798411+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:28:35.448+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='transform_dag', task_id='transform_task', run_id='manual__2025-07-08T09:28:32.798411+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:28:35.453+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=transform_dag, task_id=transform_task, run_id=manual__2025-07-08T09:28:32.798411+00:00, map_index=-1, run_start_date=2025-07-08 09:28:34.926087+00:00, run_end_date=2025-07-08 09:28:35.033956+00:00, run_duration=0.107869, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-07-08 09:28:33.292627+00:00, queued_by_job_id=1, pid=26030[0m
[[34m2025-07-08T09:28:35.617+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun transform_dag @ 2025-07-08 09:28:32.798411+00:00: manual__2025-07-08T09:28:32.798411+00:00, state:running, queued_at: 2025-07-08 09:28:32.808465+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:28:35.617+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=transform_dag, execution_date=2025-07-08 09:28:32.798411+00:00, run_id=manual__2025-07-08T09:28:32.798411+00:00, run_start_date=2025-07-08 09:28:33.274069+00:00, run_end_date=2025-07-08 09:28:35.617678+00:00, run_duration=2.343609, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:28:32.798411+00:00, data_interval_end=2025-07-08 09:28:32.798411+00:00, dag_hash=fa1ce4798c2ef7ea34c91b9b63f5793a[0m
[[34m2025-07-08T09:31:50.140+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:35:57.599+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:35:56.647696+00:00 [scheduled]>[0m
[[34m2025-07-08T09:35:57.599+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG load_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:35:57.599+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:35:56.647696+00:00 [scheduled]>[0m
[[34m2025-07-08T09:35:57.600+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:35:56.647696+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:35:57.600+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:35:56.647696+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:35:57.604+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:35:56.647696+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:35:58.930+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/load_dag.py[0m
[[34m2025-07-08T09:35:59.692+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: load_dag.load_task manual__2025-07-08T09:35:56.647696+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:36:00.244+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:35:56.647696+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:36:00.247+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=load_dag, task_id=load_task, run_id=manual__2025-07-08T09:35:56.647696+00:00, map_index=-1, run_start_date=2025-07-08 09:35:59.739410+00:00, run_end_date=2025-07-08 09:35:59.879196+00:00, run_duration=0.139786, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:35:57.599884+00:00, queued_by_job_id=1, pid=29260[0m
[[34m2025-07-08T09:36:00.415+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun load_dag @ 2025-07-08 09:35:56.647696+00:00: manual__2025-07-08T09:35:56.647696+00:00, state:running, queued_at: 2025-07-08 09:35:56.664398+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:36:00.415+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=load_dag, execution_date=2025-07-08 09:35:56.647696+00:00, run_id=manual__2025-07-08T09:35:56.647696+00:00, run_start_date=2025-07-08 09:35:57.581713+00:00, run_end_date=2025-07-08 09:36:00.415441+00:00, run_duration=2.833728, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:35:56.647696+00:00, data_interval_end=2025-07-08 09:35:56.647696+00:00, dag_hash=484afdcc2f5b74c3715faa038dbc0c2a[0m
[[34m2025-07-08T09:36:35.139+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:36:33.603835+00:00 [scheduled]>[0m
[[34m2025-07-08T09:36:35.139+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG load_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:36:35.139+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:36:33.603835+00:00 [scheduled]>[0m
[[34m2025-07-08T09:36:35.141+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:36:33.603835+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:36:35.141+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:36:33.603835+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:36:35.147+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:36:33.603835+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:36:35.925+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/load_dag.py[0m
[[34m2025-07-08T09:36:36.440+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: load_dag.load_task manual__2025-07-08T09:36:33.603835+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:36:36.922+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:36:33.603835+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:36:36.926+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=load_dag, task_id=load_task, run_id=manual__2025-07-08T09:36:33.603835+00:00, map_index=-1, run_start_date=2025-07-08 09:36:36.482310+00:00, run_end_date=2025-07-08 09:36:36.613827+00:00, run_duration=0.131517, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:36:35.140331+00:00, queued_by_job_id=1, pid=29535[0m
[[34m2025-07-08T09:36:37.091+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun load_dag @ 2025-07-08 09:36:33.603835+00:00: manual__2025-07-08T09:36:33.603835+00:00, state:running, queued_at: 2025-07-08 09:36:33.610550+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:36:37.092+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=load_dag, execution_date=2025-07-08 09:36:33.603835+00:00, run_id=manual__2025-07-08T09:36:33.603835+00:00, run_start_date=2025-07-08 09:36:35.122462+00:00, run_end_date=2025-07-08 09:36:37.092007+00:00, run_duration=1.969545, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:36:33.603835+00:00, data_interval_end=2025-07-08 09:36:33.603835+00:00, dag_hash=7a80bd394750d7b3e90b2c7784365c89[0m
[[34m2025-07-08T09:36:50.285+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:37:31.784+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:37:30.236431+00:00 [scheduled]>[0m
[[34m2025-07-08T09:37:31.784+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG load_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:37:31.784+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: load_dag.load_task manual__2025-07-08T09:37:30.236431+00:00 [scheduled]>[0m
[[34m2025-07-08T09:37:31.786+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:37:30.236431+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:37:31.786+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:37:30.236431+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:37:31.790+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'load_dag', 'load_task', 'manual__2025-07-08T09:37:30.236431+00:00', '--local', '--subdir', 'DAGS_FOLDER/load_dag.py'][0m
[[34m2025-07-08T09:37:32.544+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/load_dag.py[0m
[[34m2025-07-08T09:37:33.087+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: load_dag.load_task manual__2025-07-08T09:37:30.236431+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:37:34.092+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='load_dag', task_id='load_task', run_id='manual__2025-07-08T09:37:30.236431+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:37:34.096+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=load_dag, task_id=load_task, run_id=manual__2025-07-08T09:37:30.236431+00:00, map_index=-1, run_start_date=2025-07-08 09:37:33.143076+00:00, run_end_date=2025-07-08 09:37:33.339288+00:00, run_duration=0.196212, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:37:31.785265+00:00, queued_by_job_id=1, pid=30068[0m
[[34m2025-07-08T09:37:34.260+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun load_dag @ 2025-07-08 09:37:30.236431+00:00: manual__2025-07-08T09:37:30.236431+00:00, state:running, queued_at: 2025-07-08 09:37:30.241700+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:37:34.260+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=load_dag, execution_date=2025-07-08 09:37:30.236431+00:00, run_id=manual__2025-07-08T09:37:30.236431+00:00, run_start_date=2025-07-08 09:37:31.763329+00:00, run_end_date=2025-07-08 09:37:34.260633+00:00, run_duration=2.497304, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:37:30.236431+00:00, data_interval_end=2025-07-08 09:37:30.236431+00:00, dag_hash=7a80bd394750d7b3e90b2c7784365c89[0m
[[34m2025-07-08T09:41:50.572+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2025-07-08T09:42:52.424+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.extract_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:52.425+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:42:52.425+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.extract_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:52.426+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='extract_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-07-08T09:42:52.426+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'extract_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:52.430+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'extract_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:53.154+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:42:54.135+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.extract_task manual__2025-07-08T09:42:51.235611+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:42:56.020+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='extract_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:42:56.023+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=extract_task, run_id=manual__2025-07-08T09:42:51.235611+00:00, map_index=-1, run_start_date=2025-07-08 09:42:54.204381+00:00, run_end_date=2025-07-08 09:42:55.664368+00:00, run_duration=1.459987, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-07-08 09:42:52.425620+00:00, queued_by_job_id=1, pid=32537[0m
[[34m2025-07-08T09:42:56.500+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.transform_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:56.500+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:42:56.500+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.transform_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:56.501+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='transform_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-07-08T09:42:56.501+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'transform_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:56.505+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'transform_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:57.237+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:42:58.075+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.transform_task manual__2025-07-08T09:42:51.235611+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:42:58.710+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='transform_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:42:58.713+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=transform_task, run_id=manual__2025-07-08T09:42:51.235611+00:00, map_index=-1, run_start_date=2025-07-08 09:42:58.112260+00:00, run_end_date=2025-07-08 09:42:58.300883+00:00, run_duration=0.188623, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-07-08 09:42:56.500848+00:00, queued_by_job_id=1, pid=32559[0m
[[34m2025-07-08T09:42:58.880+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.load_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:58.880+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:42:58.880+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.load_task manual__2025-07-08T09:42:51.235611+00:00 [scheduled]>[0m
[[34m2025-07-08T09:42:58.882+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='load_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:42:58.882+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'load_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:58.887+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'load_task', 'manual__2025-07-08T09:42:51.235611+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:42:59.801+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:43:00.642+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.load_task manual__2025-07-08T09:42:51.235611+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:43:01.208+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='load_task', run_id='manual__2025-07-08T09:42:51.235611+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:43:01.212+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=load_task, run_id=manual__2025-07-08T09:42:51.235611+00:00, map_index=-1, run_start_date=2025-07-08 09:43:00.677183+00:00, run_end_date=2025-07-08 09:43:00.800854+00:00, run_duration=0.123671, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:42:58.881438+00:00, queued_by_job_id=1, pid=32582[0m
[[34m2025-07-08T09:43:01.404+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun etl_dag @ 2025-07-08 09:42:51.235611+00:00: manual__2025-07-08T09:42:51.235611+00:00, state:running, queued_at: 2025-07-08 09:42:51.245595+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:43:01.405+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=etl_dag, execution_date=2025-07-08 09:42:51.235611+00:00, run_id=manual__2025-07-08T09:42:51.235611+00:00, run_start_date=2025-07-08 09:42:52.403199+00:00, run_end_date=2025-07-08 09:43:01.405173+00:00, run_duration=9.001974, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:42:51.235611+00:00, data_interval_end=2025-07-08 09:42:51.235611+00:00, dag_hash=a51153632f51c624b9e5134d89db1fe7[0m
[[34m2025-07-08T09:44:38.828+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.extract_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:38.829+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:44:38.829+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.extract_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:38.830+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='extract_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-07-08T09:44:38.830+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'extract_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:38.835+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'extract_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:39.685+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:44:40.577+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.extract_task manual__2025-07-08T09:44:37.353600+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:44:42.502+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='extract_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:44:42.506+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=extract_task, run_id=manual__2025-07-08T09:44:37.353600+00:00, map_index=-1, run_start_date=2025-07-08 09:44:40.643604+00:00, run_end_date=2025-07-08 09:44:41.966301+00:00, run_duration=1.322697, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator, queued_dttm=2025-07-08 09:44:38.829837+00:00, queued_by_job_id=1, pid=33420[0m
[[34m2025-07-08T09:44:43.089+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.transform_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:43.089+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:44:43.089+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.transform_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:43.091+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='transform_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-07-08T09:44:43.091+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'transform_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:43.095+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'transform_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:43.832+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:44:44.678+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.transform_task manual__2025-07-08T09:44:37.353600+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:44:45.238+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='transform_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:44:45.242+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=transform_task, run_id=manual__2025-07-08T09:44:37.353600+00:00, map_index=-1, run_start_date=2025-07-08 09:44:44.720490+00:00, run_end_date=2025-07-08 09:44:44.840441+00:00, run_duration=0.119951, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-07-08 09:44:43.090249+00:00, queued_by_job_id=1, pid=33448[0m
[[34m2025-07-08T09:44:45.406+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: etl_dag.load_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:45.407+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG etl_dag has 0/16 running and queued tasks[0m
[[34m2025-07-08T09:44:45.407+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: etl_dag.load_task manual__2025-07-08T09:44:37.353600+00:00 [scheduled]>[0m
[[34m2025-07-08T09:44:45.408+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='etl_dag', task_id='load_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-07-08T09:44:45.409+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'etl_dag', 'load_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:45.412+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'etl_dag', 'load_task', 'manual__2025-07-08T09:44:37.353600+00:00', '--local', '--subdir', 'DAGS_FOLDER/challenge_dag.py'][0m
[[34m2025-07-08T09:44:46.186+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/challenge_dag.py[0m
[[34m2025-07-08T09:44:46.985+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: etl_dag.load_task manual__2025-07-08T09:44:37.353600+00:00 [queued]> on host codespaces-f9b19f[0m
[[34m2025-07-08T09:44:47.525+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='etl_dag', task_id='load_task', run_id='manual__2025-07-08T09:44:37.353600+00:00', try_number=1, map_index=-1)[0m
[[34m2025-07-08T09:44:47.528+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=etl_dag, task_id=load_task, run_id=manual__2025-07-08T09:44:37.353600+00:00, map_index=-1, run_start_date=2025-07-08 09:44:47.023147+00:00, run_end_date=2025-07-08 09:44:47.147356+00:00, run_duration=0.124209, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2025-07-08 09:44:45.407723+00:00, queued_by_job_id=1, pid=33460[0m
[[34m2025-07-08T09:44:47.696+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun etl_dag @ 2025-07-08 09:44:37.353600+00:00: manual__2025-07-08T09:44:37.353600+00:00, state:running, queued_at: 2025-07-08 09:44:37.359216+00:00. externally triggered: True> successful[0m
[[34m2025-07-08T09:44:47.697+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=etl_dag, execution_date=2025-07-08 09:44:37.353600+00:00, run_id=manual__2025-07-08T09:44:37.353600+00:00, run_start_date=2025-07-08 09:44:38.810514+00:00, run_end_date=2025-07-08 09:44:47.697057+00:00, run_duration=8.886543, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-07-08 09:44:37.353600+00:00, data_interval_end=2025-07-08 09:44:37.353600+00:00, dag_hash=a51153632f51c624b9e5134d89db1fe7[0m
